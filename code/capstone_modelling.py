# -*- coding: utf-8 -*-
"""Capstone_Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ACsv-PZ4czaIFy5A3GJ2pYBAwZMq2kUz
"""

#!!pip uninstall tensorflow
#!pip install tensorflow==2.1.0

from google.colab import drive
drive.mount('/content/drive/')

PROJECT_DIR = "/content/drive/My Drive/MachineLearning/Capstone/"
DATA_FILE_NAME = 'Data/Input Data Synthetic.xlsx'
CLEANSED_FILE_DIR = 'Data/processed/'

#import basic libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

import warnings
warnings.filterwarnings('ignore')

import keras
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, Flatten, BatchNormalization
from tensorflow.keras.models import Model, Sequential

def load_doc(filename):
  if(filename.endswith('.xlsx')):
    data_df = pd.read_excel(filename, lines=True)
  elif(filename.endswith('.csv')):
    data_df = pd.read_csv(filename, keep_default_na = False)
  return data_df

def encode_target(target):
  le = LabelEncoder()
  return le.fit_transform(target), le

def decode_prediction(pred, encoder):
  return encoder.inverse_transform(pred)

cleansed_data_df = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'cleansed_data.csv')
print('shape of Data : ', cleansed_data_df.shape)

cleansed_data_df.describe()

cleansed_data_df.head()

cleansed_data_df['assignment_group'].value_counts()

cleansed_data_df['description_length'] = cleansed_data_df['description'].str.len()
desc_max_length = cleansed_data_df['description_length'].max()
maxlen_desc_idx = cleansed_data_df['description_length'].idxmax()
print('Desc with max length ({}) at index {}:\n{}'.format(desc_max_length,maxlen_desc_idx,cleansed_data_df['description'].loc[maxlen_desc_idx]))

"""**Train & Test Data:**"""

X = cleansed_data_df['description']
y = cleansed_data_df.assignment_group
y, le = encode_target(y)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, stratify=y, random_state = 10)

X_train.head()

print('Shape of X_train : ',X_train.shape)
print('Shape of X_test : ',X_test.shape)
print('Shape of y_train : ',y_train.shape)
print('Shape of y_test : ',y_test.shape)

#pd.DataFrame(data = y_test.assignment_group.value_counts())

"""**TF-IDF Vectors as features :**"""

unigram_vocab = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'tfidf_based_unigram_tokens.csv')
bigram_vocab = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'tfidf_based_bigram_tokens.csv')
trigram_vocab = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'tfidf_based_trigram_tokens.csv')

print('unigram vocab : ', unigram_vocab.shape)
print('bigram vocab : ', bigram_vocab.shape)
print('trigram vocab : ', trigram_vocab.shape)

unigram_bigram_vocab = unigram_vocab.append(bigram_vocab)
unigram_bigram_trigram_vocab = unigram_bigram_vocab.append(trigram_vocab)

print('unigram vocab : ', unigram_vocab.shape)
print('unigram + bigram vocab : ', unigram_bigram_vocab.shape)
print('unigram + bigram + trigram vocab : ', unigram_bigram_trigram_vocab.shape)

unigram_max_features = 5000
bigram_max_features = 20000
trigram_max_features = 40000

maxlen = cleansed_data_df['description_length'].max()
embedding_size = 200

unigram_tokenizer = TfidfVectorizer(ngram_range=(1,1), max_features = unigram_max_features, vocabulary=unigram_vocab['words']).fit(X)
X_train_unigram_seq = unigram_tokenizer.transform(X_train)
X_test_unigram_seq = unigram_tokenizer.transform(X_test)
print('Vocabulary size of unigram :', len(unigram_tokenizer.vocabulary_))

bigram_tokenizer = TfidfVectorizer(ngram_range=(1,2), max_features = bigram_max_features, vocabulary=unigram_bigram_vocab['words']).fit(X)
X_train_bigram_seq = bigram_tokenizer.transform(X_train)
X_test_bigram_seq = bigram_tokenizer.transform(X_test)
print('Vocabulary size of bigram :', len(bigram_tokenizer.vocabulary_))

trigram_tokenizer = TfidfVectorizer(ngram_range=(1,3), max_features = trigram_max_features, vocabulary=unigram_bigram_trigram_vocab['words']).fit(X)
X_train_trigram_seq = trigram_tokenizer.transform(X_train)
X_test_trigram_seq = trigram_tokenizer.transform(X_test)
print('Vocabulary size of trigram :', len(trigram_tokenizer.vocabulary_))

print('Shape of X_train_unigram_seq : ',X_train_unigram_seq.shape)
print('Shape of X_test_unigram_seq : ',X_test_unigram_seq.shape)
print('Shape of X_train_bigram_seq : ',X_train_bigram_seq.shape)
print('Shape of X_test_bigram_seq : ',X_test_bigram_seq.shape)
print('Shape of X_train_trigram_seq : ',X_train_trigram_seq.shape)
print('Shape of X_test_trigram_seq : ',X_test_trigram_seq.shape)

#X_train_unigram_seq_pad = pad_sequences(X_train_seq, maxlen = 4164)
#X_test_unigram_seq_pad = pad_sequences(X_train_seq, maxlen = 4164)

"""**Model :**"""

from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble

import xgboost, numpy, textblob, string
from keras.preprocessing import text, sequence
from keras import layers, models, optimizers, regularizers
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

def train_model(model, feature_train, target_train, feature_validate, target_validate, is_neural_net=False):
    # fit the training dataset on the classifier
    model.fit(feature_train, target_train)
    # predict the labels on validation dataset
    predictions = model.predict(feature_validate)
    if is_neural_net:
        predictions = predictions.argmax(axis=-1)
    return (metrics.accuracy_score(predictions, target_validate)*100).round(3)

accuracy_result = 'Train Acc : {}\nTest Acc : {}'

accuracy_df = pd.DataFrame(columns = ['Model' , 'unigram-tfidf Accuracy', 'bigram-tfidf Accuracy' , 'trigram-tfidf Accuracy'])

accuracy_df.shape

"""**Naive Bayes :**"""

# Naive Bayes on tfidf Vectors
accuracy1 = train_model(naive_bayes.MultinomialNB(), X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(naive_bayes.MultinomialNB(), X_train_bigram_seq,  y_train, X_test_bigram_seq,  y_test)
accuracy3 = train_model(naive_bayes.MultinomialNB(), X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['Naive Bayes', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df

"""**Linear Classifier :**"""

# Logistic Regression on tfidf Vectors
accuracy1 = train_model(linear_model.LogisticRegression(), X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(linear_model.LogisticRegression(), X_train_bigram_seq,  y_train, X_test_bigram_seq,  y_test)
accuracy3 = train_model(linear_model.LogisticRegression(), X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['Linear-Logistic Regression', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df

"""**Support Vector Machines :**"""

# SVM on tfidf Vectors
accuracy1 = train_model(svm.SVC(), X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(svm.SVC(), X_train_bigram_seq,  y_train, X_test_bigram_seq,  y_test)
accuracy3 = train_model(svm.SVC(), X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['SVM Classifier', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df

"""**RandomForest Classifier :**"""

def randomforest_model(n_estimators,criterion,max_depth,min_samples_split):
  rf_model = ensemble.RandomForestClassifier(n_estimators=n_estimators, n_jobs=2, criterion = criterion,
                                          max_depth = max_depth, min_samples_split = min_samples_split, random_state=0)
  return rf_model

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(100, 1000, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [6, 10, 15]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split}

# Use the random grid to search for best hyperparameters
rf = ensemble.RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = 2)

# Fit the random search model
#rf_random.fit(X_train_trigram_seq, y_train)

#rf_random.best_params_

# RandomForest on tfidf Vectors
n_estimators = 600
criterion = 'gini'
max_depth = 100
min_samples_split = 6

unigram_rf_model = randomforest_model(n_estimators,criterion,100,6)
bigram_rf_model = randomforest_model(n_estimators,criterion,None,10)
trigram_rf_model = randomforest_model(n_estimators,criterion,None,10)

accuracy1 = train_model(unigram_rf_model, X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(bigram_rf_model, X_train_bigram_seq,  y_train, X_test_bigram_seq,  y_test)
accuracy3 = train_model(trigram_rf_model, X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['RandomForest: Esti={},Cri={},max_depth={})'.format(n_estimators,criterion,max_depth), 
                                           accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)

accuracy_df

"""**XGBoost Classifier :**"""

# XGBoost on tfidf Vectors
accuracy1 = train_model(xgboost.XGBClassifier(), X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(xgboost.XGBClassifier(), X_train_bigram_seq, y_train, X_test_bigram_seq, y_test)
accuracy3 = train_model(xgboost.XGBClassifier(), X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['XGBoost', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df

"""**KNN Classifier:**"""

# KNN Classifier on tfidf Vectors
accuracy1 = train_model(neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='brute'), X_train_unigram_seq, y_train, X_test_unigram_seq, y_test)
accuracy2 = train_model(neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='brute'), X_train_bigram_seq, y_train, X_test_bigram_seq, y_test)
accuracy3 = train_model(neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='brute'), X_train_trigram_seq, y_train, X_test_trigram_seq, y_test)

accuracy_df = accuracy_df.append(pd.Series(['KNN Classifier (k=5)', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df

"""**Simple NN Model :**"""

def create_nn_model(input_size, output_size):
    model = models.Sequential()
    
    model.add(layers.Dense(512,input_dim=input_size, kernel_initializer='random_uniform', kernel_regularizer=regularizers.l2(Lambda)))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Dropout(0.5))

    model.add(layers.Dense(256,kernel_initializer='random_uniform',kernel_regularizer=regularizers.l2(Lambda)))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Dropout(0.5))

    model.add(layers.Dense(128,kernel_initializer='random_uniform',kernel_regularizer=regularizers.l2(Lambda)))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))
    model.add(layers.Dropout(0.3))

    model.add(layers.Dense(64, kernel_initializer='random_uniform',kernel_regularizer=regularizers.l2(Lambda)))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation('relu'))

    model.add(layers.Dense(output_size, activation='softmax'))
    return model

def train_deep_model(model, feature_train, target_train, feature_validate, target_validate, epochs, batch_size, lr, Lambda, callbacks):
  model.compile(optimizer=optimizers.Adam(learning_rate=lr),
                loss='categorical_crossentropy', metrics=['accuracy'])
  history = model.fit(feature_train, target_train, validation_split=0.2, 
                      epochs=epochs, batch_size=batch_size, verbose=1,
                      callbacks = callbacks)
  predict = model.predict_classes(feature_validate)
  return (metrics.accuracy_score(predict, target_validate)*100).round(3)

#X_train_unigram_seq = X_train_unigram_seq.todense()
#X_test_unigram_seq =X_test_unigram_seq.todense()
#X_train_bigram_seq = X_train_bigram_seq.todense()
#X_test_bigram_seq =X_test_bigram_seq.todense()
#X_train_trigram_seq = X_train_trigram_seq.todense()
#X_test_trigram_seq =X_test_trigram_seq.todense()

y_train_cate = keras.utils.to_categorical(y_train)
y_test_cate = keras.utils.to_categorical(y_test)

# Simple NN with own embedding on tfidf Vectors
epochs = 50
batch_size = 128
lr = 1e-2
Lambda = 1e-5
stop = EarlyStopping(monitor="loss", patience=10, mode="min")
reduce_lr = ReduceLROnPlateau(monitor="loss", factor=0.1, patience=3, min_lr=1e-8, verbose=1, mode="min")
callback = [reduce_lr,stop]

model1 = create_nn_model(X_train_unigram_seq.shape[1], y_train_cate.shape[1])
accuracy1 = train_deep_model(model1, X_train_unigram_seq, y_train_cate, X_test_unigram_seq, y_test, epochs, batch_size, lr, Lambda, callback)

model2 = create_nn_model(X_train_bigram_seq.shape[1], y_train_cate.shape[1])
accuracy2 = train_deep_model(model2, X_train_bigram_seq, y_train_cate, X_test_bigram_seq, y_test, epochs, batch_size, lr, Lambda, callback)

model3 = create_nn_model(X_train_trigram_seq.shape[1], y_train_cate.shape[1])
accuracy3 = train_deep_model(model3, X_train_trigram_seq, y_train_cate, X_test_trigram_seq, y_test, epochs, batch_size, lr, Lambda, callback)

accuracy_df = accuracy_df.append(pd.Series(['Simple NN', accuracy1,accuracy2,accuracy3],index=accuracy_df.columns), ignore_index=True)
accuracy_df