# -*- coding: utf-8 -*-
"""Capstone_for_modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwhp1nCLXaGnInI-QLsaoYGLzwsFk0vZ
"""

from google.colab import drive
drive.mount('/content/drive/')

PROJECT_DIR = "/content/drive/My Drive/MachineLearning/Capstone/"
DATA_FILE_NAME = 'Data/Input Data Synthetic.xlsx'
CLEANSED_FILE_DIR = 'Data/processed/'

#import basic libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import nltk

import warnings
warnings.filterwarnings('ignore')

import keras
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, Flatten
from tensorflow.keras.models import Model, Sequential

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

def load_doc(filename):
  if(filename.endswith('.xlsx')):
    data_df = pd.read_excel(filename, lines=True)
  elif(filename.endswith('.csv')):
    data_df = pd.read_csv(filename, keep_default_na = False)
  return data_df

def encode_target(target):
  le = LabelEncoder()
  return le.fit_transform(target), le

def decode_prediction(pred, encoder):
  return encoder.inverse_transform(pred)

cleansed_data_df = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'cleansed_data.csv')
print('shape of Data : ', cleansed_data_df.shape)

cleansed_data_df.describe()

cleansed_data_df['assignment_group'].value_counts()

cleansed_data_df['description_length'] = cleansed_data_df['description'].str.len()
desc_max_length = cleansed_data_df['description_length'].max()
maxlen_desc_idx = cleansed_data_df['description_length'].idxmax()
print(desc_max_length)
print(maxlen_desc_idx)
print('Desc with max length ({}) at index {}:\n {}'.format(desc_max_length,maxlen_desc_idx,cleansed_data_df['description'].loc[maxlen_desc_idx]))

"""**Train & Test Data:**"""

X = cleansed_data_df['description']
y = cleansed_data_df.assignment_group
y, le = encode_target(y)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, stratify=y, random_state = 10)

max_features = 10000
maxlen = cleansed_data_df['description_length'].max()
embedding_size = 200

unigram_tokenizer = TfidfVectorizer(ngram_range=(1,1), max_features = max_features).fit(X_train)

print('Vocabulary size :', len(unigram_tokenizer.get_feature_names()))

X_train_seq = unigram_tokenizer.fit_transform(X_train)
print('Shape of X_train : ', X_train_seq.shape)

#this X_train_seq can be used as input to the model

#X_train_seq_pad = pad_sequences(X_train_seq, maxlen = 3190)

