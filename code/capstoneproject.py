# -*- coding: utf-8 -*-
"""CapstoneProject_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQGxWga5yWeHWjhgZ5S45iDRhdWUHZOE
"""

#!!pip uninstall tensorflow
#!pip install tensorflow==2.1.0
#!pip install -U pandas-profiling

from google.colab import drive
drive.mount('/content/drive/')

PROJECT_DIR = "/content/drive/My Drive/MachineLearning/Capstone/"
DATA_FILE_NAME = 'Data/Input Data Synthetic.xlsx'
CLEANSED_FILE_NAME = 'Data/cleansed_data.csv'

#import basic libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import nltk
#nltk.download('stopwords')

import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_colwidth', 0)

"""##**1. Data Loading**"""

data_df = pd.read_excel(PROJECT_DIR+DATA_FILE_NAME, lines=True)
data_df.head(20)

data_df.shape

data_df.describe()

data_df[data_df.Description.isnull()]

#Replace space in column names with '_' and to lower case for better naming conversion
data_df.columns = [c.replace(' ', '_') for c in data_df.columns]
data_df.columns = [c.lower() for c in data_df.columns]

"""Inference:
1. There are total of 8500 datasets spread across 4 dimensions.
2. As we are classifing the ticket to category based on description, caller column is of no information to us. This can be removed in the pre-processing.
3. Though there are short description & description, we will classify based on description as it provides more details on the isssue.
4. From describe, we can notice that there are unique values on the description is 7817 out of 8500. This signifies there might be some duplicates in the decription.
5. There is 1 missing value in the description which can be replaced with short description of the same.
6. There are 74 unique categories to which the tickets are classified to.
7. There seems to be lot of cleanup required in the description column.

  a. There are os related new line and line termination tags.

  b. Few description have header - received from : - which doesn't provide much information on classification.

  c. Few description also have footer note - Thanks/regards followed by name - which doesn't provide much information on classification.
  
  d. There are few encoded words in description which could be name of the persons that are encrypted owing to PII governance which needs to be handled in pre-processing.
  
  e. There are also few system drive path, software versioning number and ip addresses.

  f. Some description might have attached with evidence photos which resulted in cid: tages in the description footer.

## **2. Exploratory Data Anlysis :**

### **2.1 Pre-Requisite**
"""

#create a working copyy from the original dataframe
desc_df = data_df.copy(deep=True)

#Drop caller as it is not utilized in problem classification
desc_df = desc_df.drop(['caller'], axis=1)

desc_df.head(5)

"""### **2.2 Target Category Analysis**"""

#category_df = pd.DataFrame(data = desc_df.assignment_group.value_counts())
pd.set_option('display.max_rows', desc_df.shape[0]+1)
pd.DataFrame(data = desc_df.assignment_group.value_counts())
#category_df.head(100)

plt.figure(figsize=(22, 14))
sns.countplot(y=desc_df["assignment_group"])
plt.ylabel('Category')
plt.title('Target Count on Category')
plt.xticks(rotation=50)

#list the description which are less than 5 groupiing
temp_df = desc_df.groupby('assignment_group').filter(lambda x: len(x) < 6)
temp_df.describe()

#desc_df['assignment_group'] = 'GRP_COMMON' 
desc_df['assignment_group'] = desc_df.assignment_group.apply(lambda v: 'GRP_COMMON' if (desc_df.assignment_group.value_counts() < 6)[v] else v)

pd.set_option('display.max_rows', desc_df.shape[0]+1)
pd.DataFrame(data = desc_df.assignment_group.value_counts())

plt.figure(figsize=(22, 14))
sns.countplot(y=desc_df["assignment_group"])
plt.ylabel('Category')
plt.title('Target Count on Category')
plt.xticks(rotation=50)

"""Inference:

### **2.3 Feature Analysis:**

#### **2.3.1 Basic Analysis**
"""

#create a working copy of data set
working_desc_df = desc_df.copy(deep=True)

#working_desc_df = working_desc_df.drop(['assignment_group'], axis=1)

#check for null/NaN in the description
working_desc_df[working_desc_df.description.isnull()]

#Replace empty description with that short description
working_desc_df.description.fillna(working_desc_df.short_description, inplace=True)

#recheck for null/NaN inn\ the description
working_desc_df[working_desc_df.description.isnull()]

# find duplicate decription column
desc_duplicate_df = working_desc_df[working_desc_df.duplicated(['description'],keep=False)].sort_values('description')
print('Number of duplicate rows (inclusive) : ', desc_duplicate_df.shape[0])
desc_duplicate_df.head(50)

working_desc_df.shape

# drop duplicate decription rows
working_desc_df.drop_duplicates('description', inplace = True)

working_desc_df.shape

#ticket where short desc is greater than description
tmp_short_desc_length_df = working_desc_df[working_desc_df.short_description.str.len() > working_desc_df.description.str.len()]
tmp_short_desc_length_df.shape

def description_length_compare(x,y):
  if ((x is not np.nan) and (len(x) > len(y))):
    return x
  else:
    return y

working_desc_df['description'] = working_desc_df.apply(lambda x: description_length_compare(x.short_description, x.description), axis=1)
working_desc_df.head()

tmp_short_desc_length_df = working_desc_df[working_desc_df.short_description.str.len() > working_desc_df.description.str.len()]
tmp_short_desc_length_df.shape

#to get words and their count in the dataset
def tokenize_counter(data):
  words_bow_series = data.str.split(expand=True).unstack().value_counts()
  words_bow_df = pd.DataFrame({'words':words_bow_series.index, 'count':words_bow_series.values})
  return words_bow_df;

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

#method to create the n-gram from the corpus along with frequency of occurance
def get_top_n_ngram(corpus, ngram_range = (1,1), top=None, vectorizer = 'count'):
    if(vectorizer.lower() == 'tfidf'):
      vec = TfidfVectorizer(ngram_range=ngram_range).fit(corpus)
    else :
      vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    ngram_df = pd.DataFrame(words_freq[:top], columns = ['words' , 'freq'])
    return ngram_df

#bar plot for word cound plotting
def word_count_plot(xData, yData, numofwords, sort = 'asc'):
  if sort == 'asc':
    start = 0
    end = numofwords
  else:
    end = len(yData)
    start = end - numofwords

  trace = sns.barplot(
      y=yData.iloc[start:end],
      x=xData.iloc[start:end])
  return trace

#word cloud implementation to understannd word importance and frequency
from wordcloud import WordCloud
def generate_wordcloud(corpus_df, stopwords='', mask=None, max_words=100, max_font_size=100, figure_size=(24.0,16.0), 
                   title = None, title_size=24, image_color=False, ):

  wordcloud = WordCloud(background_color='black',
                    stopwords = stopwords,
                    max_words = max_words,
                    max_font_size = max_font_size, 
                    random_state = 42,
                    width=800, 
                    height=600,collocations = False,
                    mask = mask).generate_from_frequencies(dict(corpus_df))
  return wordcloud

"""#### **2.3.2 Before Cleaning**"""

desc_bow_df = get_top_n_ngram(working_desc_df['description'])
print('Length of tokens : ', len(desc_bow_df))

desc_bow_df

desc_wordcloud = generate_wordcloud(desc_bow_df.values.tolist())

plt.figure(figsize=(20, 8))
plt.suptitle('Word Count Plots for Description (prior to cleaning)',fontsize=20)
plt.subplot(1, 2, 1)
plt.title('Frequent Occuring words in Description')
word_count_plot(desc_bow_df['freq'], desc_bow_df['words'], 30, 'asc')
plt.subplot(1, 2, 2)
plt.title('WordCloud for Description')
plt.imshow(desc_wordcloud)
plt.axis("off")

"""Combine above steps into single method defination for re-use."""

#method to plot frequency & wordcloud
def plot_count_cloud_graph(wordcount, wordcloud, title):
  plt.figure(figsize=(20, 8))
  plt.suptitle(title,fontsize=20)
  plt.subplot(1, 2, 1)
  plt.title('Frequent Occuring words in Description')
  word_count_plot(wordcount['freq'], wordcount['words'], 30, 'asc')
  plt.subplot(1, 2, 2)
  plt.title('WordCloud for Description')
  plt.imshow(desc_wordcloud)
  plt.axis("off")

#method to generate bow & wordcloud
def generate_bow_wordcloud(text):
  bow_df = get_top_n_ngram(text)
  wordcloud = generate_wordcloud(bow_df.values.tolist())
  return bow_df, wordcloud

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['description'])
#desc_bow_df = generate_bow_wordcloud(working_desc_df['description'])
#desc_bow_df

##to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (prior to cleaning)')
#print top 5 description
working_desc_df.head()

"""Inference:
1. Notice too many common words like - the, to, in, is - exists.
2. Notice the existance of special characeters - ?, monitoring_tool@company.com - the dataset

Lets do carry some pre processing on the data to clean up

#### **2.3.3 Data Cleaning Analysis**

Pre-Processing
1. Remove excape characters - new line
2. Remove received from header & footer tags
2. Remove the image attachment reference in footer
4. Remove any email id reference
4. Remove Punctuation
6. Remove stop words
7. Check frequent words with respect to problem context
"""

#remove os line break tags
def remove_line_break(text):
  text = str(text)
  data = text.replace('\r\n', '\n')
  data = data.replace('\r', '\n')
  return data

#remove some header and footers in the descriptionn
def remove_exclusion_text(text, exclusionText, line_breaker):
  #print('data : ', data)
  data = [line for line in text.split(line_breaker) if not exclusionText in line]
  return line_breaker.join(data)

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = " ".join([word for word in text.split('\n')])
  return data

#word starting with following tags can be excluded as they dont difine problem statement
exclusion_text_tags = ['received from', 'cid:image', 'importance:', 
                       'email:','subject:','to:', 'sent:', 'from:', 
                       'approved by', ' user id :','name :','mailto:', '(yes/no/na)']

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (in- mid of cleansing)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

#method to convert text to lowercase
def to_lower (text):
  return text.lower()

#remove some header and footers in the descriptionn
import re
def remove_email(text):
  re_mail = re.compile('\S+@+\S+.?')
  data = [re_mail.sub(' ', word) for word in text.split()]
  return ' '.join(data)

def remove_underscore(text):
  text = text.replace("'",'')
  data = []
  for word in text.split():
    if('job_' in word):
      data.append(word)
    else:
      data.append(word.replace('_', ' '))    
  return ' '.join(data)

#replace punctutions with white space
def remove_punctuation(text):
    punctuation = string.punctuation
    punctuation = punctuation.replace('_','') #elimanate underscore
    return text.translate(str.maketrans(punctuation, ' '*len(punctuation)))

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = " ".join([word for word in text.split('\n')])
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (in mid of cleansing)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

#strip more than 1 leading or trailing white spaces
def strip_addl_whitespace(text):
    return " ".join([word for word in text.split() if word.strip()])

# remove any numbers in the dataset
def remove_non_alpha(text):
  return " ".join([word for word in text.split() if word.isalpha()])

# remove any numbers in the dataset
def remove_digits(text):
  #re_digits = re.compile(" \d+")
  #data = [re_digits.sub(' ', word) for word in text.split()]
  #return " ".join(data)
  return " ".join([word for word in text.split() if not word.isdigit()])

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  #print(data)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = " ".join([word for word in text.split('\n')])
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_digits(data)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (with stopwords)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

"""Stop Words:"""

#fetch stopwods from nltk lib
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')

#method to exclude the stopwords from the corpura and also generate exclude word cout
from collections import Counter
from collections import OrderedDict

removed_words=[]
def remove_stopwords(text, stopwords):
  global removed_words
  #global removed_words_counter 
  global removed_words_df
  data = []
  for word in text.split():
    if not word in stopwords:
      data.append(word)
    else:
      removed_words.append(word)
  removed_words_counter = Counter(removed_words)
  removed_words_counter = OrderedDict(sorted(removed_words_counter.items(), key = lambda x: x[1], reverse=True))
  removed_words_df = pd.DataFrame(removed_words_counter.items(),columns=['words','count'])
  return ' '.join(data)

def clean_text(text, exclusionTagList, stop_words=None):
  data = str(text)
  data = remove_line_break(data)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_digits(data)
  data = remove_stopwords(data, stop_words)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

plt.figure(figsize=(20, 8))
plt.title('Frequent Occuring words in Description')
word_count_plot(desc_bow_df['freq'], desc_bow_df['words'], 30, 'desc')

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
print('Vocabulary Size : ', len(desc_bow_df))
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (without  stopwords)')

"""Lets explore the removed words"""

plt.figure(figsize=(20, 8))
plt.title('Word Count Plots for Removed Words',fontsize=20)
word_count_plot(removed_words_df['count'],removed_words_df['words'], 30, 'asc')

addl_stopwords = ['yes', 'please', 'hello', 'name','yes','best','kind'] #added
not_stopwords = ['not', 'no', 'nor'] #remove
#added caller name to stop words
caller_names = pd.DataFrame(data_df.caller.str.split().tolist()).stack().reset_index()[0].tolist()

#to add addl stop words to the nltk list
stop_words.extend(addl_stopwords)
stop_words.extend(caller_names)
#remove non stopwords from NLTK stopwords list
stop_words = [word for word in stop_words if word not in not_stopwords]

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
print('Vocabulary Size : ', len(desc_bow_df))
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (without updated stopwords)')

plt.figure(figsize=(20, 8))
plt.title('Word Count Plots for Removed Words',fontsize=20)
word_count_plot(removed_words_df['count'],removed_words_df['words'], 30, 'asc')

"""### **2.3.4 n-gram Analysis**"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

#method to create the n-gram from the corpus along with frequency of occurance
def get_top_n_ngram(corpus, ngram_range = (1,1), top=None, vectorizer = 'count'):
    if(vectorizer.lower() == 'tfidf'):
      vec = TfidfVectorizer(ngram_range=ngram_range).fit(corpus)
    else :
      vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)  
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    ngram_df = pd.DataFrame(words_freq[:top], columns = ['words' , 'freq'])
    return ngram_df

#word cloud implementation to understannd word importance and frequency
from wordcloud import WordCloud
def generate_wordcloud(corpus_df, stopwords='', mask=None, max_words=100, max_font_size=100, figure_size=(24.0,16.0), 
                   title = None, title_size=24, image_color=False, ):
 
  wordcloud = WordCloud(background_color='black',
                    stopwords = stopwords,
                    max_words = max_words,
                    max_font_size = max_font_size, 
                    random_state = 42,
                    width=800, 
                    height=600,collocations = False,
                    mask = mask).generate_from_frequencies(dict(corpus_df))
  return wordcloud

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags,[]))
working_desc_df['cleansed_desc_wo_stopwords'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

#unigram_count_df = get_top_n_ngram(working_desc_df['cleansed_desc'],(1,1), 20, 'count')
#unigram_tfidf_df = get_top_n_ngram(working_desc_df['cleansed_desc'],(1,1), 20, 'count')
unigram_count_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(1,1), None, 'count')
unigram_tfidf_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(1,1), None, 'tfidf')

print('Total no of uni-grams from corpus - tfidf : ', unigram_tfidf_df_wo_stopwords.shape[0])
print('Total no of uni-grams from corpus - count : ', unigram_count_df_wo_stopwords.shape[0])

plt.figure(figsize=(20, 10))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(2, 2, 1)
plt.title('Most Frequest uni-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(unigram_tfidf_df_wo_stopwords['freq'],unigram_tfidf_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 2)
plt.title('Least Frequent uni-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(unigram_tfidf_df_wo_stopwords['freq'],unigram_tfidf_df_wo_stopwords['words'], 20, 'desc')
plt.subplot(2, 2, 3)
plt.title('Most Frequesnt uni-grams without stopwords using count',fontsize=20)
word_count_plot(unigram_count_df_wo_stopwords['freq'],unigram_count_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 4)
plt.title('Least Frequent uni-grams without stopwords using Count',fontsize=20)
word_count_plot(unigram_count_df_wo_stopwords['freq'],unigram_count_df_wo_stopwords['words'], 20, 'desc')
plt.tight_layout()

bigram_count_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(2,2), None, 'count')
bigram_tfidf_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(2,2), None, 'tfidf')

print('Total no of bi-grams from corpus - tfidf : ', bigram_tfidf_df_wo_stopwords.shape[0])
print('Total no of bi-grams from corpus - count : ', bigram_count_df_wo_stopwords.shape[0])

plt.figure(figsize=(20, 10))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(2, 2, 1)
plt.title('Most Frequest bi-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(bigram_tfidf_df_wo_stopwords['freq'],bigram_tfidf_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 2)
plt.title('Least Frequent bi-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(bigram_tfidf_df_wo_stopwords['freq'],bigram_tfidf_df_wo_stopwords['words'], 20, 'desc')
plt.subplot(2, 2, 3)
plt.title('Most Frequesnt bi-grams without stopwords using count',fontsize=20)
word_count_plot(bigram_count_df_wo_stopwords['freq'],bigram_count_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 4)
plt.title('Least Frequent bi-grams without stopwords using Count',fontsize=20)
word_count_plot(bigram_count_df_wo_stopwords['freq'],bigram_count_df_wo_stopwords['words'], 20, 'desc')
plt.tight_layout()

trigram_count_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(3,3), None, 'count')
trigram_tfidf_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(3,3), None, 'tfidf')

print('Total no of tri-grams from corpus - tfidf : ', trigram_tfidf_df_wo_stopwords.shape[0])
print('Total no of tri-grams from corpus - count : ', trigram_count_df_wo_stopwords.shape[0])

plt.figure(figsize=(20, 10))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(2, 2, 1)
plt.title('Most Frequest tri-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(trigram_tfidf_df_wo_stopwords['freq'],trigram_tfidf_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 2)
plt.title('Least Frequent tri-grams without stopwords using tf-idf',fontsize=20)
word_count_plot(trigram_tfidf_df_wo_stopwords['freq'],trigram_tfidf_df_wo_stopwords['words'], 20, 'desc')
plt.subplot(2, 2, 3)
plt.title('Most Frequent tri-grams without stopwords using count',fontsize=20)
word_count_plot(trigram_count_df_wo_stopwords['freq'],trigram_count_df_wo_stopwords['words'], 20, 'asc')
plt.subplot(2, 2, 4)
plt.title('Least Frequent tri-grams without stopwords using Count',fontsize=20)
word_count_plot(trigram_count_df_wo_stopwords['freq'],trigram_count_df_wo_stopwords['words'], 20, 'desc')
plt.tight_layout()

#trigram_count_df_wo_stopwords

unigram_wordcloud = generate_wordcloud(unigram_count_df_wo_stopwords.values.tolist())
bigram_wordcloud = generate_wordcloud(bigram_count_df_wo_stopwords.values.tolist())
trigram_wordcloud = generate_wordcloud(trigram_count_df_wo_stopwords.values.tolist())

plt.figure(figsize=(22, 10))

plt.subplot(1, 3, 1)
plt.imshow(unigram_wordcloud)
plt.title('WordCloud for uni-gram')
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(bigram_wordcloud)
plt.title('WordCloud for bi-gram')
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(trigram_wordcloud)
plt.title('WordCloud for tri-gram')
plt.axis("off")

"""## **3. Pre-Processing**

1. Load document from directory
2. Null check on features
3. Cleansing the data as explained in EDA
4. Save back to directory
"""

from sklearn.preprocessing import LabelEncoder

def load_doc(filename):
  if(filename.endswith('.xlsx')):
    data_df = pd.read_excel(filename, lines=True)
  elif(filename.endswith('.csv')):
    data_df = pd.read_csv(filename)
  return data_df

#remove os line break tags
def remove_line_break(text):
  text = str(text)
  data = text.replace('\r\n', '\n')
  data = data.replace('\r', '\n')
  return data

#remove some header and footers in the descriptionn
def remove_exclusion_text(text, exclusionText, line_breaker):
  data = [line for line in text.split(line_breaker) if not exclusionText in line]
  return line_breaker.join(data)

#method to convert text to lowercase
def to_lower (text):
  return text.lower()

#remove some header and footers in the descriptionn
import re
def remove_email(text):
  re_mail = re.compile('\S+@+\S+.?')
  data = [re_mail.sub(' ', word) for word in text.split()]
  return ' '.join(data)

def remove_underscore(text):
  text = text.replace("'",'')
  data = []
  for word in text.split():
    if('job_' in word):
      data.append(word)
    else:
      data.append(word.replace('_', ' '))    
  return ' '.join(data)

#replace punctutions with white space
def remove_punctuation(text):
    punctuation = string.punctuation
    punctuation = punctuation.replace('_','') #elimanate underscore
    return text.translate(str.maketrans(punctuation, ' '*len(punctuation)))

#strip more than 1 leading or trailing white spaces
def strip_addl_whitespace(text):
    return " ".join([word for word in text.split() if word.strip()])

# remove any numbers in the dataset
def remove_non_alpha(text):
  return " ".join([word for word in text.split(" ") if word.isalpha()])

# remove any numbers in the dataset
def remove_digits(text):
  #re_digits = re.compile(" \d+")
  #data = [re_digits.sub(' ', word) for word in text.split()]
  #return " ".join(data)
  return " ".join([word for word in text.split() if not word.isdigit()])


#method to exclude the stopwords from the corpura and also generate exclude word cout
from collections import Counter
from collections import OrderedDict

removed_words=[]
def remove_stopwords(text, stopwords):
  global removed_words
  #global removed_words_counter 
  global removed_words_df
  data = []
  for word in text.split():
    if not word in stopwords:
      data.append(word)
    else:
      removed_words.append(word)
  removed_words_counter = Counter(removed_words)
  removed_words_counter = OrderedDict(sorted(removed_words_counter.items(), key = lambda x: x[1], reverse=True))
  removed_words_df = pd.DataFrame(removed_words_counter.items(),columns=['words','count'])
  return ' '.join(data)

def clean_text(text, exclusionTagList, stop_words=None):
  data = remove_line_break(text)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = " ".join([word for word in text.split('\n')])
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_digits(data)
  data = remove_stopwords(data, stop_words)
  return data

def encode_target(target):
  le = LabelEncoder()
  return le.fit_transform(target), le

def decode_prediction(pred, encoder):
  return encoder.inverse_transform(pred)

import os
def save_doc(df, dir, filename):
  if not os.path.isdir(dir):
    os.mkdir(dir)
  df.to_csv(dir+filename, index = False, header=True)

def generate_stopwords(new_stopwords, remove_stopwords):
  stop_words = nltk.corpus.stopwords.words('english')
  #to add addl stop words to the nltk list
  stop_words.extend(new_stopwords)
  #remove non stopwords from NLTK stopwords list
  stop_words = [word for word in stop_words if word not in remove_stopwords]
  return stop_words

def description_length_compare(x,y):
  if ((x is not np.nan) and (len(x) > len(y))):
    return x
  else:
    return y

def pre_process_pipeline(srcFile, features=[], target=[], stop_words=[]):
  data_df = load_doc(srcFile)
  data_df.columns = [c.replace(' ', '_') for c in data_df.columns]
  data_df.columns = [c.lower() for c in data_df.columns]
  #columns_to_be_droped = [c for c in list(data_df.columns) if not c in (features + target)]
  #print('columns_to_be_droped : ', columns_to_be_droped)
  #data_df = data_df.drop(columns_to_be_droped, axis=1)
  #data_df['combined_desc'] = data_df['short_description'] + ' ' + data_df['description']
  data_df.description.fillna(data_df.short_description, inplace=True)
  data_df.short_description.fillna("", inplace=True)
  data_df['description'] = data_df.apply(lambda x: description_length_compare(x.short_description, x.description), axis=1)

  #added caller name to stop words
  caller_names = pd.DataFrame(data_df.caller.str.split().tolist()).stack().reset_index()[0].tolist()
  stop_words.extend(caller_names)

  for column in features:
    print('pre-processing column : ',column)
    data_df[column] = data_df[column].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

  data_df.drop_duplicates('description', inplace = True)
  data_df['combined_desc'] = data_df['short_description'] + ' ' + data_df['description']
  data_df = data_df[['short_description','description','combined_desc','caller','assignment_group']]
  return data_df

features=['description','short_description']
target= ['assignment_group']
srcFile = PROJECT_DIR+DATA_FILE_NAME
CLEANSED_FILE_DIR = 'Data/processed/'

addl_stopwords = ['yes', 'please', 'hello', 'name','yes','best','kind','hi','na']
not_stopwords = ['not', 'no', 'nor']
stop_words = generate_stopwords(addl_stopwords, not_stopwords)

#word starting with following tags can be excluded as they dont difine problem statement
exclusion_text_tags = ['received from', 'cid:image', 'importance:', 
                       'email:','subject:','to:', 'sent:', 'from:', 
                       'approved by', ' user id :','name :','mailto:', '(yes/no/na)']

cleansed_data_df = pre_process_pipeline(srcFile, features, target, stop_words)

cleansed_data_df.describe()

# find duplicate decription column
desc_duplicate_df = cleansed_data_df[cleansed_data_df.duplicated(['description'],keep=False)].sort_values('description')
print('Number of duplicate rows (inclusive) : ', desc_duplicate_df.shape[0])
desc_duplicate_df.head(50)

save_doc(cleansed_data_df, PROJECT_DIR+CLEANSED_FILE_DIR, 'cleansed_data.csv')
save_doc(removed_words_df, PROJECT_DIR+CLEANSED_FILE_DIR, 'removed_words.csv')

#cleansed_data_df.describe()

"""**Vocabulary :**"""

unigram_vocal = get_top_n_ngram(cleansed_data_df['description'],(1,1), None, 'tfidf')
bigram_vocal = get_top_n_ngram(cleansed_data_df['description'],(2,2), None, 'tfidf')
trigram_vocal = get_top_n_ngram(cleansed_data_df['description'],(3,3), None, 'tfidf')

print('Size of uni-gram Vocabulary : ', unigram_vocal.shape[0])
print('Size of bi-gram Vocabulary : ', bigram_vocal.shape[0])
print('Size of tri-gram Vocabulary : ', trigram_vocal.shape[0])

#unigram_vocal.info()

# keep vocal with a min occurrence
min_freq = 0.2
unigram_tokens = unigram_vocal[unigram_vocal['freq'] > min_freq]
bigram_tokens = bigram_vocal[bigram_vocal['freq'] > min_freq]
trigram_tokens = trigram_vocal[trigram_vocal['freq'] > min_freq]

unigram_tokens

print('Size of uni-gram Tokens : ', unigram_tokens.shape[0])
print('Size of bi-gram Tokens : ', bigram_tokens.shape[0])
print('Size of tri-gram Tokens : ', trigram_tokens.shape[0])

save_doc(unigram_tokens, PROJECT_DIR+CLEANSED_FILE_DIR, 'tfidf_based_unigram_tokens.csv')
save_doc(bigram_tokens, PROJECT_DIR+CLEANSED_FILE_DIR, 'tfidf_based_bigram_tokens.csv')
save_doc(trigram_tokens, PROJECT_DIR+CLEANSED_FILE_DIR, 'tfidf_based_trigram_tokens.csv')

#cleansed_data_df.reset_index()

unigram_tokens[unigram_tokens['words'].isnull()]

#unigram_tokens[1475:1525]

