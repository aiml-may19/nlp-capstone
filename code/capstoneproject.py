# -*- coding: utf-8 -*-
"""CapstoneProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQGxWga5yWeHWjhgZ5S45iDRhdWUHZOE
"""

#!!pip uninstall tensorflow
#!pip install tensorflow==2.1.0

from google.colab import drive
drive.mount('/content/drive/')

PROJECT_DIR = "/content/drive/My Drive/MachineLearning/Capstone/"
DATA_FILE_NAME = 'Data/Input Data Synthetic.xlsx'
CLEANSED_FILE_NAME = 'Data/cleansed_data.csv'

#import basic libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import nltk
#nltk.download('stopwords')

import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_colwidth', 0)

"""##**1. Data Loading**"""

data_df = pd.read_excel(PROJECT_DIR+DATA_FILE_NAME, lines=True)
data_df.head(20)

data_df.shape

data_df.describe()

data_df[data_df.Description.isnull()]

#Replace space in column names with '_' and to lower case for better naming conversion
data_df.columns = [c.replace(' ', '_') for c in data_df.columns]
data_df.columns = [c.lower() for c in data_df.columns]

"""Inference:
1. There are total of 8500 datasets spread across 4 dimensions.
2. As we are classifing the ticket to category based on description, caller column is of no information to us. This can be removed in the pre-processing.
3. Though there are short description & description, we will classify based on description as it provides more details on the isssue.
4. From describe count, we can notice that there are some missing values in the descriptions columnns.
5. There is 1 missing value in the description which can be replaced with short description of the same.
6. There are 74 unique categories to which the tickets are classified to.
7. There seems to be lot of cleanup required in the description column.

  a. There are os related new line and line termination tags.

  b. Few description have header - received from : - which doesn't provide much information on classification.

  c. Few description also have footer note - Thanks/regards follwed by name - which doesn't provide much information on classification.
  
  d. There are few encoded words in description which could be name of the persons that are encrypted owing to PII governance which needs to be handled in pre-processing.
  
  e. There are also few system drive path, software versioning number and ip addresses.

  f. Some description might have attached with evidence photos which resulted in cid: tages in the description footer.

## **2. Exploratory Data Anlysis :**

### **2.1 Pre-Requestic**
"""

#create a working copyy from the original dataframe
desc_df = data_df.copy(deep=True)

#Drop caller as it is not utilized in problem classification
desc_df = desc_df.drop(['caller'], axis=1)

#check for null/NaN inn the description
desc_df[desc_df.description.isnull()]

#Replace empty description with that short description
desc_df.description.fillna(desc_df.short_description, inplace=True)

#recheck for null/NaN in description
desc_df[desc_df.description.isnull()]

desc_df.head(5)

"""### **2.2 Target Category Analysis**"""

##category_df.index = description_df.index
#category_df = pd.DataFrame(data = description_df.copy('assignment_group'), columns=['category'])
#category_df = description_df.assignment_group.value_counts()
#category_df.head()

#labelencoder = LabelEncoder()
#data_df['cate_assignment_group'] = labelencoder.fit_transform(data_df['assignment_group'])

plt.figure(figsize=(22, 14))
sns.countplot(y=desc_df["assignment_group"])
plt.ylabel('Category')
plt.title('Target Count on Category')
plt.xticks(rotation=50)

"""Inference:

### **2.3 Feature Analysis:**

#### **2.3.1 Basic Analysis**
"""

#create a working copy of data set
working_desc_df = desc_df.copy(deep=True)

working_desc_df = working_desc_df.drop(['short_description', 'assignment_group'], axis=1)

working_desc_df.head(10)

#to get words and their count in the dataset
def tokenize_counter(data):
  words_bow_series = data.str.split(expand=True).unstack().value_counts()
  words_bow_df = pd.DataFrame({'words':words_bow_series.index, 'count':words_bow_series.values})
  return words_bow_df;

#bar plot for word cound plotting
def word_count_plot(xData, yData, numofwords, sort = 'asc'):
  if sort == 'asc':
    start = 0
    end = numofwords
  else:
    end = len(description_bow['count'])
    start = end - numofwords

  trace = sns.barplot(
      y=yData.iloc[start:end],
      x=xData.iloc[start:end])
  return trace

#word cloud implementation to understannd word importance and frequency
from wordcloud import WordCloud
def generate_wordcloud(corpus, stopwords='', mask=None, max_words=100, max_font_size=100, figure_size=(24.0,16.0), 
                   title = None, title_size=24, image_color=False, ):
  desc_words = ' '
  #combine all text in the corpus into string of words
  for line in corpus:
   desc_words = desc_words + line + ' '

  wordcloud = WordCloud(background_color='black',
                    stopwords = stopwords,
                    #max_words = max_words,
                    max_font_size = max_font_size, 
                    random_state = 42,
                    width=800, 
                    height=600,collocations = False,
                    mask = mask).generate(str(desc_words))
  return wordcloud

"""#### **2.3.2 Before Cleaning**"""

desc_bow_df = tokenize_counter(working_desc_df['description'])
print('Length of tokens : ', len(desc_bow_df))

desc_wordcloud = generate_wordcloud(working_desc_df['description'])
len(desc_wordcloud.words_)

plt.figure(figsize=(20, 8))
plt.suptitle('Word Count Plots for Description (prior to cleaning)',fontsize=20)
plt.subplot(1, 2, 1)
plt.title('Frequent Occuring words in Description')
word_count_plot(desc_bow_df['count'], desc_bow_df['words'], 30, 'asc')
plt.subplot(1, 2, 2)
plt.title('WordCloud for Description')
plt.imshow(desc_wordcloud)
plt.axis("off")

"""Combine above steps into single method defination for re-use."""

#method to plot frequency & wordcloud
def plot_count_cloud_graph(wordcount, wordcloud, title):
  plt.figure(figsize=(20, 8))
  plt.suptitle(title,fontsize=20)
  plt.subplot(1, 2, 1)
  plt.title('Frequent Occuring words in Description')
  word_count_plot(wordcount['count'], wordcount['words'], 30, 'asc')
  plt.subplot(1, 2, 2)
  plt.title('WordCloud for Description')
  plt.imshow(desc_wordcloud)
  plt.axis("off")

#method to generate bow & wordcloud
def generate_bow_wordcloud(text):
  bow_df = tokenize_counter(text)
  wordcloud = generate_wordcloud(text)
  return bow_df, wordcloud

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['description'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (prior to cleaning)')
#print top 5 description
working_desc_df.head()

"""Inference:
1. Notice too many common words like - the, to, in, is - exists.
2. Notice the existance of special characeters - ?, monitoring_tool@company.com - the dataset

Lets do carry some pre processing on the data to clean up

#### **2.3.3 Data Cleaning Analysis**

Pre-Processing
1. Remove excape characters - new line
2. Remove received from header & footer tags
2. Remove the image attachment reference in footer
4. Remove any email id reference
4. Remove Punctuation
6. Remove stop words
7. Check frequent words with respect to problem context
"""

#remove os line break tags
def remove_line_break(text):
  text = str(text)
  data = text.replace('\r\n', '\n')
  data = data.replace('\r', '\n')
  return data

#remove some header and footers in the descriptionn
def remove_exclusion_text(text, exclusionText, line_breaker):
  data = [line for line in text.split(line_breaker) if not exclusionText in line]
  return ' '.join(data)

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  #print(data)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  return data

#word starting with following tags can be excluded as they dont difine problem statement
exclusion_text_tags = ['received from', 'cid:image', 'importance:', 
                       'email:','subject:','to:', 'sent:', 'from:', 
                       'approved by', ' user id :','name :','mailto:']

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (in- mid of cleansing)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

#method to convert text to lowercase
def to_lower (text):
  return text.lower()

#remove some header and footers in the descriptionn
import re
def remove_email(text):
  re_mail = re.compile('\S+@+\S+.?')
  data = [re_mail.sub(' ', word) for word in text.split()]
  return ' '.join(data)

def remove_underscore(text):
    text = text.replace('_',' ')
    text = text.replace("'",'')
    return text

#replace punctutions with white space
def remove_punctuation(text):
    return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description (in mid of cleansing)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

#strip more than 1 leading or trailing white spaces
def strip_addl_whitespace(text):
    return " ".join([word for word in text.split() if word.strip()])

# remove any numbers in the dataset
def remove_non_alpha(text):
  return " ".join([word for word in text.split(" ") if word.isalpha()])

def clean_text(text, exclusionTagList):
  data = remove_line_break(text)
  #print(data)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_non_alpha(data)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
#to print vocal size from Bag Of Words created
print('Vocabulary Size : {} \n'.format(len(desc_bow_df)))
#plot the frequency $ cloud plot
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (with stopwords)')
#print top 5 description
working_desc_df['cleansed_desc'].head()

"""Stop Words:"""

#fetch stopwods from nltk lib
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')

#method to exclude the stopwords from the corpura and also generate exclude word cout
from collections import Counter
from collections import OrderedDict

removed_words=[]
def remove_stopwords(text, stopwords):
  global removed_words
  #global removed_words_counter 
  global removed_words_df
  data = []
  for word in text.split():
    if not word in stopwords:
      data.append(word)
    else:
      removed_words.append(word)
  removed_words_counter = Counter(removed_words)
  removed_words_counter = OrderedDict(sorted(removed_words_counter.items(), key = lambda x: x[1], reverse=True))
  removed_words_df = pd.DataFrame(removed_words_counter.items(),columns=['words','count'])
  return ' '.join(data)

def clean_text(text, exclusionTagList, stop_words=None):
  data = str(text)
  data = remove_line_break(data)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_non_alpha(data)
  data = remove_stopwords(data, stop_words)
  return data

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
print('Vocabulary Size : ', len(desc_bow_df))
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (without  stopwords)')

"""Lets explore the removed words"""

plt.figure(figsize=(20, 8))
plt.title('Word Count Plots for Removed Words',fontsize=20)
word_count_plot(removed_words_df['count'],removed_words_df['words'], 30, 'asc')

addl_stopwords = ['yes', 'please', 'hello', 'name','yes','best','kind']
not_stopwords = ['not', 'no', 'nor']

#to add addl stop words to the nltk list
stop_words.extend(addl_stopwords)
#remove non stopwords from NLTK stopwords list
stop_words = [word for word in stop_words if word not in not_stopwords]

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

desc_bow_df, desc_wordcloud = generate_bow_wordcloud(working_desc_df['cleansed_desc'])
print('Vocabulary Size : ', len(desc_bow_df))
plot_count_cloud_graph(desc_bow_df, desc_wordcloud, 'Word Count Plots for Description post cleansing (without updated stopwords)')

plt.figure(figsize=(20, 8))
plt.title('Word Count Plots for Removed Words',fontsize=20)
word_count_plot(removed_words_df['count'],removed_words_df['words'], 30, 'asc')

"""### **2.3.4 n-gram Analysis**"""

from sklearn.feature_extraction.text import CountVectorizer

#method to create the n-gram from the corpus along with frequency of occurance
def get_top_n_ngram(corpus, ngram_range = (1,1), top=None):
    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    ngram_df = pd.DataFrame(words_freq[:top], columns = ['n-gram_words' , 'count'])
    return ngram_df

working_desc_df['cleansed_desc'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags,[]))
working_desc_df['cleansed_desc_wo_stopwords'] = working_desc_df['description'].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))

unigram_df = get_top_n_ngram(working_desc_df['cleansed_desc'],(1,1), 20)
unigram_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(1,1), 20)

plt.figure(figsize=(20, 8))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(1, 2, 1)
plt.title('Word Count Plots for uni-grams with stopwords',fontsize=20)
word_count_plot(unigram_df['count'],unigram_df['n-gram_words'], 30, 'asc')
plt.subplot(1, 2, 2)
plt.title('Word Count Plots for uni-grams without stopwords',fontsize=20)
word_count_plot(unigram_df_wo_stopwords['count'],unigram_df_wo_stopwords['n-gram_words'], 30, 'asc')
plt.tight_layout()

bigram_df = get_top_n_ngram(working_desc_df['cleansed_desc'],(2,2), 20)
bigram_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(2,2), 20)

plt.figure(figsize=(20, 8))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(1, 2, 1)
plt.title('Word Count Plots for bi-grams with stopwords',fontsize=20)
word_count_plot(bigram_df['count'],bigram_df['n-gram_words'], 30, 'asc')
plt.subplot(1, 2, 2)
plt.title('Word Count Plots for bi-grams without stopwords',fontsize=20)
word_count_plot(bigram_df_wo_stopwords['count'],bigram_df_wo_stopwords['n-gram_words'], 30, 'asc')
plt.tight_layout()

trigram_df = get_top_n_ngram(working_desc_df['cleansed_desc'],(3,3), 20)
trigram_df_wo_stopwords = get_top_n_ngram(working_desc_df['cleansed_desc_wo_stopwords'],(3,3), 20)

plt.figure(figsize=(20, 8))
#plt.suptitle('Word Count Plots for uni-grams',fontsize=20)
plt.subplot(1, 2, 1)
plt.title('Word Count Plots for tri-grams with stopwords',fontsize=20)
word_count_plot(trigram_df['count'],trigram_df['n-gram_words'], 30, 'asc')
plt.subplot(1, 2, 2)
plt.title('Word Count Plots for tri-grams without stopwords',fontsize=20)
word_count_plot(trigram_df_wo_stopwords['count'],trigram_df_wo_stopwords['n-gram_words'], 30, 'asc')
plt.tight_layout()

"""## **3. Pre-Processing**

1. Load document from directory
2. Null check on features
3. Cleansing the data as explained in EDA
4. Save back to directory
"""

from sklearn.preprocessing import LabelEncoder

def load_doc(filename):
  return pd.read_excel(filename, lines=True)

#method to convert text to lowercase
def to_lower (text):
  return text.lower()

#remove some header and footers in the descriptionn
import re
def remove_email(text):
  re_mail = re.compile('\S+@+\S+.?')
  data = [re_mail.sub(' ', word) for word in text.split()]
  return ' '.join(data)

def remove_underscore(text):
    text = text.replace('_',' ')
    text = text.replace("'",'')
    return text

 #replace punctutions with white space
def remove_punctuation(text):
    return text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))

#strip more than 1 leading or trailing white spaces
def strip_addl_whitespace(text):
    return " ".join([word for word in text.split() if word.strip()])

# remove any numbers in the dataset
def remove_non_alpha(text):
  return " ".join([word for word in text.split(" ") if word.isalpha()])

#method to exclude the stopwords from the corpura and also generate exclude word cout
from collections import Counter
from collections import OrderedDict

removed_words=[]
def remove_stopwords(text, stopwords):
  global removed_words
  #global removed_words_counter 
  global removed_words_df
  data = []
  for word in text.split():
    if not word in stopwords:
      data.append(word)
    else:
      removed_words.append(word)
  removed_words_counter = Counter(removed_words)
  removed_words_counter = OrderedDict(sorted(removed_words_counter.items(), key = lambda x: x[1], reverse=True))
  removed_words_df = pd.DataFrame(removed_words_counter.items(),columns=['words','count'])
  return ' '.join(data)

def clean_text(text, exclusionTagList, stop_words=None):
  data = remove_line_break(text)
  for excl_txt in exclusionTagList:
    data = remove_exclusion_text(data, excl_txt, '\n')
  data = to_lower(data)
  data = remove_email(data)
  data = remove_underscore(data)
  data = remove_punctuation(data)
  data = strip_addl_whitespace(data)
  data = remove_non_alpha(data)
  data = remove_stopwords(data, stop_words)
  return data

def encode_target(target):
  le = LabelEncoder()
  return le.fit_transform(target), le

def decode_prediction(pred, encoder):
  return encoder.inverse_transform(pred)

import os
def save_doc(df, dir, filename):
  if not os.path.isdir(dir):
    os.mkdir(dir)
  df.to_csv(dir+filename, index = False, header=True)

def generate_stopwords(new_stopwords, remove_stopwords):
  stop_words = nltk.corpus.stopwords.words('english')
  #to add addl stop words to the nltk list
  stop_words.extend(new_stopwords)
  #remove non stopwords from NLTK stopwords list
  stop_words = [word for word in stop_words if word not in remove_stopwords]
  return stop_words

def pre_process_pipeline(srcFile, features=[], target=[], stop_words=[]):
  data_df = load_doc(srcFile)
  data_df.columns = [c.replace(' ', '_') for c in data_df.columns]
  data_df.columns = [c.lower() for c in data_df.columns]
  columns_to_be_droped = [c for c in list(data_df.columns) if not c in (features + target)]
  print('columns_to_be_droped : ', columns_to_be_droped)
  data_df = data_df.drop(columns_to_be_droped, axis=1)
  data_df['combined_desc'] = data_df['short_description'] + ' ' + data_df['description']
  data_df.description.fillna(data_df.short_description, inplace=True)
  data_df = data_df[['short_description','description','combined_desc','assignment_group']]
  features.append('combined_desc') 

  for column in features:
    print('pre-processing column : ',column)
    data_df[column] = data_df[column].apply(lambda text : clean_text(text, exclusion_text_tags, stop_words))
  
  #le = LabelEncoder()
  #for t in target:
    #data_df[t] = le.fit_transform(data_df[t])
  #return data_df, le
  return data_df

features=['short_description','description']
target= ['assignment_group']
srcFile = PROJECT_DIR+DATA_FILE_NAME
CLEANSED_FILE_DIR = 'Data/processed/'

addl_stopwords = ['yes', 'please', 'hello', 'name','yes','best','kind']
not_stopwords = ['not', 'no', 'nor']
stop_words = generate_stopwords(addl_stopwords, not_stopwords)

cleansed_data_df = pre_process_pipeline(srcFile, features, target, stop_words)

save_doc(data_df, PROJECT_DIR+CLEANSED_FILE_DIR, 'cleansed_data.csv')
save_doc(removed_words_df, PROJECT_DIR+CLEANSED_FILE_DIR, 'removed_words.csv')

cleansed_data_df

