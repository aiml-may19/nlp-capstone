# -*- coding: utf-8 -*-
"""Capstone_input_sequence.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cHyVx6i9AjnDDlbP58n5ECQHD9mUStTX
"""

#!!pip uninstall tensorflow
#!pip install tensorflow==2.1.0

from google.colab import drive
drive.mount('/content/drive/')

PROJECT_DIR = "/content/drive/My Drive/MachineLearning/Capstone/"
DATA_FILE_NAME = 'Data/Input Data Synthetic.xlsx'
CLEANSED_FILE_DIR = 'Data/processed/'

#import basic libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import nltk
from sklearn.preprocessing import LabelEncoder
#nltk.download('stopwords')

import warnings
warnings.filterwarnings('ignore')

import keras
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, Flatten
from tensorflow.keras.models import Model, Sequential

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

def load_doc(filename):
  if(filename.endswith('.xlsx')):
    data_df = pd.read_excel(filename, lines=True)
  elif(filename.endswith('.csv')):
    data_df = pd.read_csv(filename, keep_default_na = False)
  return data_df

def encode_target(target):
  le = LabelEncoder()
  return le.fit_transform(target), le

def decode_prediction(pred, encoder):
  return encoder.inverse_transform(pred)

cleansed_data_df = load_doc(PROJECT_DIR+CLEANSED_FILE_DIR+'cleansed_data.csv')
print('shape of Data : ', cleansed_data_df.shape)

cleansed_data_df.describe()

cleansed_data_df.head()

cleansed_data_df['assignment_group'].value_counts()

cleansed_data_df['description_length'] = cleansed_data_df['description'].str.len()
desc_max_length = cleansed_data_df['description_length'].max()
maxlen_desc_idx = cleansed_data_df['description_length'].idxmax()
print('Desc with max length ({}) at index {}:\n{}'.format(desc_max_length,maxlen_desc_idx,cleansed_data_df['description'].loc[maxlen_desc_idx]))

"""**Train & Test Data:**"""

X = cleansed_data_df['description']
y = cleansed_data_df.assignment_group
y, le = encode_target(y)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, stratify=y, random_state = 10)

X_train.head()

print('Shape of X_train : ',X_train.shape)
print('Shape of X_test : ',X_test.shape)
print('Shape of y_train : ',y_train.shape)
print('Shape of y_test : ',y_test.shape)

#pd.DataFrame(data = y_test.assignment_group.value_counts())

max_features = 10000
maxlen = cleansed_data_df['description_length'].max()
embedding_size = 200

"""**TF-IDF Vectors as features :**"""

unigram_tokenizer = TfidfVectorizer(ngram_range=(1,1), max_features = max_features).fit(X)
X_train_unigram_seq = unigram_tokenizer.transform(X_train)
X_test_unigram_seq = unigram_tokenizer.transform(X_test)
print('Vocabulary size of unigram :', len(unigram_tokenizer.get_feature_names()))

bigram_tokenizer = TfidfVectorizer(ngram_range=(1,2), max_features = max_features).fit(X)
X_train_bigram_seq = bigram_tokenizer.transform(X_train)
X_test_bigram_seq = bigram_tokenizer.transform(X_test)
print('Vocabulary size of bigram :', len(bigram_tokenizer.get_feature_names()))

trigram_tokenizer = TfidfVectorizer(ngram_range=(1,3), max_features = max_features).fit(X)
X_train_trigram_seq = trigram_tokenizer.transform(X_train)
X_test_trigram_seq = trigram_tokenizer.transform(X_test)
print('Vocabulary size of trigram :', len(trigram_tokenizer.get_feature_names()))

print('Shape of X_train_unigram_seq : ',X_train_unigram_seq.shape)
print('Shape of X_test_unigram_seq : ',X_test_unigram_seq.shape)
print('Shape of X_train_bigram_seq : ',X_train_bigram_seq.shape)
print('Shape of X_test_bigram_seq : ',X_test_bigram_seq.shape)
print('Shape of X_train_trigram_seq : ',X_train_trigram_seq.shape)
print('Shape of X_test_trigram_seq : ',X_test_trigram_seq.shape)

#X_train_unigram_seq_pad = pad_sequences(X_train_seq, maxlen = 4164)
#X_test_unigram_seq_pad = pad_sequences(X_train_seq, maxlen = 4164)

#above sequences can be used for modeling